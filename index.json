
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I‚Äôm Xitong Zhang, currently a machine learning engineer at Qualcomm working on GenAI applications. I obtained my Ph.D. in Computational Mathematics at Michigan State University. I‚Äôm honored to be under the guidance of Dr. Wang, Rongrong and Dr. Hirn, Matthew. My academic journey has been deeply rooted in the study of machine learning and its wide-ranging applications.\n","date":1696695892,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1696695892,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I‚Äôm Xitong Zhang, currently a machine learning engineer at Qualcomm working on GenAI applications. I obtained my Ph.D. in Computational Mathematics at Michigan State University. I‚Äôm honored to be under the guidance of Dr.","tags":null,"title":"Xitong Zhang","type":"authors"},{"authors":["Avrajit Ghosh","Xitong Zhang","Kenneth K Sun","Qing Qu","Saiprasad Ravishankar","Rongrong Wang"],"categories":[],"content":"","date":1717777492,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717777492,"objectID":"2658cb23513dd293d6f91f833ce93c0c","permalink":"https://ZhangXit.github.io/publication/oes/","publishdate":"2024-06-07T12:24:52-04:00","relpermalink":"/publication/oes/","section":"publication","summary":"We introduce Optimal Eye Surgeon (OES), a framework for pruning and training deep image generator networks. Typically, untrained deep convolutional networks, which include image sampling operations, serve as effective image priors. However, they tend to overfit to noise in image restoration tasks due to being overparameterized. OES addresses this by adaptively pruning networks at random initialization to a level of underparameterization. This process effectively captures low-frequency image components even without training, by just masking. When trained to fit noisy image, these pruned subnetworks, which we term Sparse-DIP, resist overfitting to noise. This benefit arises from underparameterization and the regularization effect of masking, constraining them in the manifold of image priors. We demonstrate that subnetworks pruned through OES surpass other leading pruning methods, such as the Lottery Ticket Hypothesis, which is known to be suboptimal for image recovery tasks. Our extensive experiments demonstrate the transferability of OES-masks and the characteristics of sparse-subnetworks for image generation.","tags":[],"title":"Optimal Eye Surgeon: Finding image priors through sparse generators at initialization","type":"publication"},{"authors":["Guangliang Liu","Milad Afshari","Xitong Zhang","Zhiyu Xue","Avrajit Ghosh","Bidhan Bashyal","Rongrong Wang","Kristen Johnson"],"categories":[],"content":"","date":1717691092,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717691092,"objectID":"1e226cdda56b8dd8529004751160c8b2","permalink":"https://ZhangXit.github.io/publication/forget/","publishdate":"2024-06-06T12:24:52-04:00","relpermalink":"/publication/forget/","section":"publication","summary":"While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model can be approximated by the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs' bias levels from stages of pretraining and debiasing.","tags":[],"title":"Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness","type":"publication"},{"authors":["Xitong Zhang","Ismail R Alkhouri","Rongrong Wang"],"categories":[],"content":"","date":1715012692,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715012692,"objectID":"896616baf99ba7a83c856d5906b487c3","permalink":"https://ZhangXit.github.io/publication/compression/","publishdate":"2024-05-06T12:24:52-04:00","relpermalink":"/publication/compression/","section":"publication","summary":"Deep Neural Networks (DNNs) have achieved remarkable success in addressing many previously unsolvable tasks. However, the storage and computational requirements associated with DNNs pose a challenge for deploying these trained models on resource-limited devices. Therefore, a plethora of compression and pruning techniques have been proposed in recent years. Low-rank decomposition techniques are among the approaches most utilized to address this problem. Compared to post-training compression, compression-promoted training is still under-explored. In this paper, we present a theoretically-justified novel approach, termed Low-Rank Induced Training (LoRITa), that promotes low-rankness through the composition of linear layers and compresses by using singular value truncation. This is achieved without the need to change the structure at inference time or require constrained and/or additional optimization, other than the standard weight decay regularization. Moreover, LoRITa eliminates the need to (i) initialize with pre-trained models and (ii) specify rank selection prior to training. Our experimental results (i) demonstrate the effectiveness of our approach using MNIST on Fully Connected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 on Convolutional Neural Networks, and (ii) illustrate that we achieve either competitive or SOTA results when compared to leading structured pruning methods in terms of FLOPs and parameters drop.","tags":[],"title":"Structure-Preserving Network Compression Via Low-Rank Induced Training Through Linear Layers Composition","type":"publication"},{"authors":["Holly R Steach","Siddharth Viswanath","Yixuan He","Xitong Zhang","Natalia Ivanova","Matthew Hirn","Michael Perlmutter","Smita Krishnaswamy"],"categories":[],"content":"","date":1714407892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714407892,"objectID":"2b59266d3913d5d61411b7bfe65fe1c9","permalink":"https://ZhangXit.github.io/publication/infer/","publishdate":"2024-04-29T12:24:52-04:00","relpermalink":"/publication/infer/","section":"publication","summary":"The ability to measure gene expression at single-cell resolution has elevated our understanding of how biological features emerge from complex and interdependent networks at molecular, cellular, and tissue scales. As technologies have evolved that complement scRNAseq measurements with things like single-cell proteomic, epigenomic, and genomic information, it becomes increasingly apparent how much biology exists as a product of multimodal regulation. Biological processes such as transcription, translation, and post-translational or epigenetic modification impose both energetic and specific molecular demands on a cell and are therefore implicitly constrained by the metabolic state of the cell. While metabolomics is crucial for defining a holistic model of any biological process, the chemical heterogeneity of the metabolome makes it particularly difficult to measure, and technologies capable of doing this at single-cell resolution are far behind other multiomics modalities. To address these challenges, we present GEFMAP (Gene Expression-based Flux Mapping and Metabolic Pathway Prediction), a method based on geometric deep learning for predicting flux through reactions in a global metabolic network using transcriptomics data, which we ultimately apply to scRNAseq. GEFMAP leverages the natural graph structure of metabolic networks to learn both a biological objective for each cell and estimate a mass-balanced relative flux rate for each reaction in each cell using novel deep learning models.","tags":[],"title":"Inferring Metabolic States from Single Cell Transcriptomic Data via Geometric Deep Learning","type":"publication"},{"authors":["Guangliang Liu","Zhiyu Xue","Xitong Zhang","Kristen Johnson","Rongrong Wang"],"categories":[],"content":"","date":1696695892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696695892,"objectID":"05bf3ea4a46a178cc984480230bd3ccc","permalink":"https://ZhangXit.github.io/publication/pactuning/","publishdate":"2023-10-07T12:24:52-04:00","relpermalink":"/publication/pactuning/","section":"publication","summary":"Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve good generalization performance and avoid overfitting, techniques such as data augmentation and pruning are often applied. However, adding these regularizations necessitates heavy tuning of the hyperparameters of optimization algorithms, such as the popular Adam optimizer. In this paper, we propose a two-stage fine-tuning method, PAC-tuning, to address this optimization challenge. First, based on PAC training, PAC-tuning directly minimizes the PAC-Bayes generalization bound to learn proper parameter distribution variances. Second, PAC-tuning modifies the gradient by injecting the noise variances learned in the first stage into the model parameters during training, a variation of perturbed gradient descent (PGD). However, in the few-shot setting, minimizing the PAC-Bayes generalization bound of a overparametrized model such as PLMs and injecting noise into it is a nontrivial task. Our experimental results across 5 GLUE benchmark tasks demonstrate the PAC-tuning framework successfully fulfills the challenging fine-tuning tasks and outperforms strong baseline methods by a visible margin, further confirming the potential to apply PAC training for any other settings where the Adam optimizer is currently used for training.","tags":[],"title":"PAC-tuning: Fine-tuning Pre-trained Language Models with PAC-driven Perturbed Gradient Descent","type":"publication"},{"authors":[],"categories":null,"content":"","date":1696449641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696449641,"objectID":"d91f96721e3041478eeb6b9fe9acb3a5","permalink":"https://ZhangXit.github.io/talk/unlocking-tuning-free-generalization-minimizing-the-pac-bayes-bound-with-trainable-priors/","publishdate":"2023-10-10T15:59:41-04:00","relpermalink":"/talk/unlocking-tuning-free-generalization-minimizing-the-pac-bayes-bound-with-trainable-priors/","section":"event","summary":"","tags":[],"title":"Unlocking Tuning-Free Generalization: Minimizing the PAC-Bayes Bound with Trainable Priors","type":"event"},{"authors":["Xitong Zhang","Avrajit Ghosh","Guangliang Liu","Rongrong Wang"],"categories":[],"content":"","date":1696170598,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696170598,"objectID":"cffe98e6c29a767446bf4243c41171e7","permalink":"https://ZhangXit.github.io/publication/preprint-pac-bayes/","publishdate":"2023-10-10T10:29:58-04:00","relpermalink":"/publication/preprint-pac-bayes/","section":"publication","summary":"Our proposed tuning-free PAC-Bayes training framework achieves test performance comparable to that of SGD/Adam, even when the latter are optimized through a complete grid search and supplemented with additional regularization terms.","tags":[],"title":"Unlocking Tuning-free Generalization: Minimizing the PAC-Bayes Bound with Trainable Priors","type":"publication"},{"authors":[],"categories":null,"content":"","date":1691780452,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691780452,"objectID":"63ef6fc5cea64681c4275ffb0d7c5f28","permalink":"https://ZhangXit.github.io/talk/explicit-and-implicit-regularization-in-machine-learning/","publishdate":"2023-10-10T15:53:52-04:00","relpermalink":"/talk/explicit-and-implicit-regularization-in-machine-learning/","section":"event","summary":"","tags":[],"title":"Explicit and Implicit Regularization in Machine Learning","type":"event"},{"authors":["Xitong Zhang","Shusil Dangi","Sandesh Ghimire","Ravi Dayana"],"categories":[],"content":"","date":1691690932,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691690932,"objectID":"6db12a24ef58dff3468829d9de2c8f17","permalink":"https://ZhangXit.github.io/project/diffusion/","publishdate":"2023-08-10T14:08:52-04:00","relpermalink":"/project/diffusion/","section":"project","summary":"During the Qualcomm 2023 summer internship, we initially delved into unsupervised inpainting and outpainting using the RePaint framework. We enhanced this by introducing a smoothing guidance for improved sampling. To speed up the diffusion process, we then investigated both unsupervised and supervised Wavelet Diffusion Models.","tags":["Demo"],"title":"Inpainting and Outpainting with Diffusion Models","type":"project"},{"authors":["Zhichao Hou","Xitong Zhang","Wei Wang","Charu C. Aggarwal","Xiaorui Liu"],"categories":[],"content":"","date":1685809305,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685809305,"objectID":"65509bc25a1ab274bc80219ec9f90b41","permalink":"https://ZhangXit.github.io/publication/directrobust/","publishdate":"2023-06-03T12:21:45-04:00","relpermalink":"/publication/directrobust/","section":"publication","summary":"The existing research on robust Graph Neural Networks (GNNs) fails to acknowledge the significance of directed graphs in providing rich information about networks' inherent structure. This work presents the first investigation into the robustness of GNNs in the context of directed graphs, aiming to harness the profound trust implications offered by directed graphs to bolster the robustness and resilience of GNNs. Our study reveals that existing directed GNNs are not adversarially robust. In pursuit of our goal, we introduce a new and realistic directed graph attack setting and propose an innovative, universal, and efficient message-passing framework as a plug-in layer to significantly enhance the robustness of GNNs. Combined with existing defense strategies, this framework achieves outstanding clean accuracy and state-of-the-art robust performance, offering superior defense against both transfer and adaptive attacks. The findings in this study reveal a novel and promising direction for this crucial research area. The code will be made publicly available upon the acceptance of this work.","tags":[],"title":"Can Directed Graph Neural Networks be Adversarially Robust?","type":"publication"},{"authors":[],"categories":null,"content":"","date":1684765802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684765802,"objectID":"8e5649e3a4efd7cd51c21710f7bf07eb","permalink":"https://ZhangXit.github.io/talk/introduction-of-spatiotemporal-data-miningapplications-in-traffic-and-geophysics/","publishdate":"2023-10-10T15:48:02-04:00","relpermalink":"/talk/introduction-of-spatiotemporal-data-miningapplications-in-traffic-and-geophysics/","section":"event","summary":"","tags":[],"title":"Introduction of Spatiotemporal Data Mining‚ÄìApplications in Traffic and Geophysics","type":"event"},{"authors":["Avrajit Ghosh","He Lyu","Xitong Zhang","Rongrong Wang"],"categories":[],"content":"","date":1675355314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675355314,"objectID":"cc34b9001a162215c365c0bf0446056f","permalink":"https://ZhangXit.github.io/publication/implicit/","publishdate":"2023-10-10T12:28:34-04:00","relpermalink":"/publication/implicit/","section":"publication","summary":"It is well known that the finite step-size in Gradient Descent (GD) implicitly regularizes solutions to flatter minima. A natural question to ask is Does the momentum parameter play a role in implicit regularization in Heavy-ball (H.B) momentum accelerated gradient descent (GD+M)? To answer this question, first, we show that the discrete H.B momentum update (GD+M) follows a continuous trajectory induced by a modified loss, which consists of an original loss and an implicit regularizer. Then, we show that this implicit regularizer for (GD+M) is stronger than that of (GD) by factor, thus explaining why (GD+M) shows better generalization performance and higher test accuracy than (GD). Furthermore, we extend our analysis to the stochastic version of gradient descent with momentum (SGD+M) and characterize the continuous trajectory of the update of (SGD+M) in a pointwise sense. We explore the implicit regularization in (SGD+M) and (GD+M) through a series of experiments validating our theory.","tags":[],"title":"Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent","type":"publication"},{"authors":["Yanhua Liu","Xitong Zhang","Ilya Tsvankin","Youzuo Lin"],"categories":[],"content":"","date":1670517454,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670517454,"objectID":"b5826e3fe95583153048d30a1b0cb8e4","permalink":"https://ZhangXit.github.io/publication/uncertainco2/","publishdate":"2022-12-08T12:37:34-04:00","relpermalink":"/publication/uncertainco2/","section":"publication","summary":"Monitoring changes inside a reservoir in real time is crucial for the success of CO2 injection and long-term storage. Machine learning (ML) is well-suited for real-time CO2 monitoring because of its computational efficiency. However, most existing applications of ML yield only one prediction (i.e., the expectation) for a given input, which may not properly reflect the distribution of the testing data, if it has a shift with respect to that of the training data. The Simultaneous Quantile Regression (SQR) method can estimate the entire conditional distribution of the target variable of a neural network via pinball loss. Here, we incorporate this technique into seismic inversion for purposes of CO2 monitoring. The uncertainty map is then calculated pixel by pixel from a particular prediction interval around the median. We also propose a novel data-augmentation method by sampling the uncertainty to further improve prediction accuracy. The developed methodology is tested on synthetic Kimberlina data, which are created by the Department of Energy and based on a CO2 capture and sequestration (CCS) project in California. The results prove that the proposed network can estimate the subsurface velocity rapidly and with sufficient resolution. Furthermore, the computed uncertainty quantifies the prediction accuracy. The method remains robust even if the testing data are distorted due to problems in the field data acquisition. Another test demonstrates the effectiveness of the developed data-augmentation method in increasing the spatial resolution of the estimated velocity field and in reducing the prediction error.","tags":[],"title":"Enhanced prediction accuracy with uncertainty quantification in monitoring CO2 sequestration using convolutional neural networks","type":"publication"},{"authors":["Chengyuan Deng","Shihang Feng","Hanchen Wang","Xitong Zhang","Peng Jin","Yinan Feng","Qili Zeng","Yinpeng Chen","Youzuo Lin"],"categories":[],"content":"","date":1670346451,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670346451,"objectID":"618d3814bc581e6ac31cc93ddcdc1aaf","permalink":"https://ZhangXit.github.io/publication/openfwi/","publishdate":"2022-12-06T13:07:31-04:00","relpermalink":"/publication/openfwi/","section":"publication","summary":"OpenFWI dataset","tags":[],"title":"OpenFWI: Large-scale Multi-structural Benchmark Datasets for Full Waveform Inversion","type":"publication"},{"authors":["Chengyuan Deng","Shihang Feng","Hanchen Wang","Xitong Zhang","Peng Jin","Yinan Feng","Qili Zeng","Yinpeng Chen","Youzuo Lin"],"categories":null,"content":"","date":1670284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670284800,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://ZhangXit.github.io/project/example/","publishdate":"2022-12-06T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"OpenFWI is a collection of large-scale, multi-structural benchmark datasets for machine learning driven seismic FWI. We release twelve datasets synthesized from different priors, including one 3D dataset. We also provide baseline experimental results with four deep learning methods, InversionNet, VelocityGAN, UPFWI and InversionNet3D. OpenFWI is the first open-source platform to facilitate data-driven FWI research. It will be actively developed and the datasets are expected to evolve.","tags":["benchmark"],"title":"OpenFWI Datasets","type":"project"},{"authors":["Xitong Zhang","Will Reichard-Flynn","Miao Zhang","Matthew Hirn","Youzuo Lin"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1666742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666742400,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://ZhangXit.github.io/publication/journal-article/","publishdate":"2022-10-26T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Earthquake characterization by graph neural networks.","tags":["Source Themes"],"title":"Spatiotemporal Graph Convolutional Networks for Earthquake Source Characterization","type":"publication"},{"authors":["Shihang Feng","Peng Jin","Xitong Zhang","Yinpeng Chen","David Alumbaugh","Michael Commer","Youzuo Lin"],"categories":[],"content":"","date":1660582232,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660582232,"objectID":"52eacf7551ec3f399c618ed190bc51bb","permalink":"https://ZhangXit.github.io/publication/extremeweak/","publishdate":"2022-08-15T12:50:32-04:00","relpermalink":"/publication/extremeweak/","section":"publication","summary":"Multi-physics inversion plays a critical role in geophysics. It has been widely used to simultaneously infer various geophysical properties~(such as velocity and conductivity). Among those inversion problems, some are explicitly governed by partial differential equations~(PDEs), while others are not. Without explicit governing equations, conventional physical-based inversion techniques are not feasible and data-driven inversion requires expensive full labels. To overcome this issue, we proposed a new data-driven multi-physics inversion technique with extremely weak supervision. Our key finding is that the pseudo labels can be constructed by learning the local relationship among geophysical properties at very sparse locations. We explore the multi-physics inversion problem from two distinct measurements~(seismic and electromagnetic data) to three geophysical properties~(velocity, conductivity, and CO2 saturation) with synthetic data based on the Kimberlina storage reservoir in California.  Our results show that we are able to invert for properties without explicit governing equations. Moreover, the labeled data on three geophysical properties can be significantly reduced by 50 times~(from 100 down to only 2 locations).","tags":[],"title":"Weakly Supervised Inversion of Multi-physics Data for Geophysical Properties","type":"publication"},{"authors":["Peng Jin","Xitong Zhang","Yinpeng Chen","Sharon Xiaolei Huang","Zicheng Liu","Youzuo Lin"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. ","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://ZhangXit.github.io/publication/conference-paper/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"This paper investigates unsupervised learning of Full-Waveform Inversion (FWI) by integrating PDE and CNN in a loop.","tags":[],"title":"Unsupervised learning of full-waveform inversion: Connecting CNN and partial differential equation in a loop","type":"publication"},{"authors":["Yixuan He","Xitong Zhang","Junjie Huang","Benedek Rozemberczki","Mihai Cucuringu","Gesine Reinert"],"categories":[],"content":"","date":1645553015,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645553015,"objectID":"b9ce289bf3f4d535b7f2ddcc3d48dfb7","permalink":"https://ZhangXit.github.io/project/pygsd/","publishdate":"2022-02-22T14:03:35-04:00","relpermalink":"/project/pygsd/","section":"project","summary":"PyTorch Geometric Signed Directed is a signed/directed graph neural network extension library for PyTorch Geometric. It builds on open-source deep-learning and graph processing libraries. PyTorch Geometric Signed Directed consists of various signed and directed geometric deep learning, embedding, and clustering methods from a variety of published research papers and selected preprints.","tags":["package"],"title":"PyTorch Geometric Signed Directed","type":"project"},{"authors":["Yixuan He","Xitong Zhang","Junjie Huang","Benedek Rozemberczki","Mihai Cucuringu","Gesine Reinert"],"categories":[],"content":"","date":1645551303,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645551303,"objectID":"8ab377fc20d7fe42077d33392b99d472","permalink":"https://ZhangXit.github.io/publication/pygsd/","publishdate":"2022-02-22T13:35:03-04:00","relpermalink":"/publication/pygsd/","section":"publication","summary":"Networks are ubiquitous in many real-world applications (e.g., social networks encoding trust/distrust relationships, correlation networks arising from time series data). While many networks are signed or directed, or both, there is a lack of unified software packages on graph neural networks (GNNs) specially designed for signed and directed networks. In this paper, we present PyTorch Geometric Signed Directed (PyGSD), a software package which fills this gap. Along the way, we also provide a brief review surveying typical tasks, loss functions and evaluation metrics in the analysis of signed and directed networks, discuss data used in related experiments, provide an overview of methods proposed, and evaluate the implemented methods with experiments. The deep learning framework consists of easy-to-use GNN models, synthetic and real-world data, as well as task-specific evaluation metrics and loss functions for signed and directed networks. As an extension library for PyG, our proposed software is maintained with open-source releases, detailed documentation, continuous integration, unit tests and code coverage checks.","tags":[],"title":"PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs","type":"publication"},{"authors":["Yuxin Yang","Xitong Zhang","Qiang Guan","Youzuo Lin"],"categories":[],"content":"","date":1642524833,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642524833,"objectID":"2c9c07e13e5f6c6339e62b591fe24f54","permalink":"https://ZhangXit.github.io/publication/dataaug/","publishdate":"2022-01-18T12:53:53-04:00","relpermalink":"/publication/dataaug/","section":"publication","summary":"Deep learning and data-driven approaches have shown great potential in scientific domains. The promise of data-driven techniques relies on the availability of a large volume of high-quality training datasets. Due to the high cost of obtaining data through expensive physical experiments, instruments, and simulations, data augmentation techniques for scientific applications have emerged as a new direction for obtaining scientific data recently. However, existing data augmentation techniques originating from computer vision yield physically unacceptable data samples that are not helpful for the domain problems that we are interested in. In this article, we develop new data augmentation techniques based on convolutional neural networks. Specifically, our generative models leverage different physics knowledge (such as governing equations, observable perception, and physics phenomena) to improve the quality of the synthetic data. To validate the effectiveness of our data augmentation techniques, we apply them to solve a subsurface seismic full-waveform inversion using simulated CO2 leakage data. Our interest is to invert for subsurface velocity models associated with very small CO2 leakage. We validate the performance of our methods using comprehensive numerical tests. Via comparison and analysis, we show that data-driven seismic imaging can be significantly enhanced by using our data augmentation techniques. Particularly, the imaging quality has been improved by 15% in test scenarios of general-sized leakage and 17% in small-sized leakage when using an augmented training set obtained with our techniques.","tags":[],"title":"Making Invisible Visible: Data-Driven Seismic Inversion With Spatio-Temporally Constrained Data Augmentation","type":"publication"},{"authors":["Xitong Zhang","Yixuan He","Nathan Brugnone","Michael Perlmutter","Matthew Hirn"],"categories":null,"content":"","date":1638804661,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638804661,"objectID":"3e254040e3a7c1d9b7d9c86219c2769e","permalink":"https://ZhangXit.github.io/publication/magnet/","publishdate":"2021-12-06T11:31:01-04:00","relpermalink":"/publication/magnet/","section":"publication","summary":"We proposed a directed graph neural network based on a complex Hermitian matrix known as the Magnetic Laplacian.","tags":[],"title":"Magnet: A neural network for directed graphs","type":"publication"},{"authors":["Junbong Jang","Chuangqi Wang","Xitong Zhang","Hee June Choi","Xiang Pan","Bolun Lin","Yudong Yu","Carly Whittle","Madison Ryan","Yenyu Chen","Kwonmoo Lee"],"categories":[],"content":"","date":1637601915,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637601915,"objectID":"05d400ad6ba103ece195522acde21fe2","permalink":"https://ZhangXit.github.io/publication/cellreport/","publishdate":"2021-11-22T13:25:15-04:00","relpermalink":"/publication/cellreport/","section":"publication","summary":"To accurately segment cell edges and quantify cellular morphodynamics from live-cell imaging data, we developed a deep learning-based pipeline termed MARS-Net (multiple-microscopy-type-based accurate and robust segmentation network). MARS-Net utilizes transfer learning and data from multiple types of microscopy to localize cell edges with high accuracy. For effective training on distinct types of live-cell microscopy, MARS-Net comprises a pretrained VGG19 encoder with U-Net decoder and dropout layers. We trained MARS-Net on movies from phase-contrast, spinning-disk confocal, and total internal reflection fluorescence microscopes. MARS-Net produced more accurate edge localization than the neural network models trained with single-microscopy-type datasets. We expect that MARS-Net can accelerate the studies of cellular morphodynamics by providing accurate pixel-level segmentation of complex live-cell datasets.","tags":[],"title":"A deep learning-based segmentation pipeline for profiling cellular morphodynamics using multiple types of live cell microscopy","type":"publication"},{"authors":["Shihang Feng","Xitong Zhang","Brendt Wohlberg","Neill P Symons","Youzuo Lin"],"categories":[],"content":"","date":1633713663,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633713663,"objectID":"71a0cca7d38f47e2aa04cea30dc6cf7e","permalink":"https://ZhangXit.github.io/publication/connectdots/","publishdate":"2021-10-08T13:21:03-04:00","relpermalink":"/publication/connectdots/","section":"publication","summary":"4-D seismic imaging has been widely used in CO2 sequestration projects to monitor the fluid flow in the volumetric subsurface region that is not sampled by wells. Ideally, realtime monitoring and near-future forecasting would provide site operators with great insights to understand the dynamics of the subsurface reservoir and assess any potential risks. However, due to obstacles such as high deployment cost, availability of acquisition equipment, exclusion zones around surface structures, only very sparse seismic imaging data can be obtained during monitoring. That leads to an unavoidable and growing knowledge gap over time. The operator needs to understand the fluid flow throughout the project lifetime and the seismic data are only available at a limited number of times. This is insufficient for understanding reservoir behavior. To overcome those challenges, we have developed spatio-temporal neuralnetwork-based models that can produce high-fidelity interpolated or extrapolated images effectively and efficiently. Specifically, our models are built on an autoencoder, and incorporate the long short-term memory (LSTM) structure with a new loss function regularized by optical flow. We validate the performance of our models using real 4-D post-stack seismic imaging data acquired at the Sleipner CO2 sequestration field. We employ two different strategies in evaluating our models. Numerically, we compare our models with different baseline approaches using classic pixelbased metrics. We also conduct a blind survey and collect a total of 20 responses from domain experts to evaluate the quality of data generated by our models. Via both numerical and expert evaluation, we conclude that our models can produce high-quality 2-D/3-D seismic imaging data at a reasonable cost, offering the possibility of real-time monitoring or even near-future forecasting of the CO2 storage reservoir.","tags":[],"title":"Connect the Dots: In Situ 4-D Seismic Monitoring of CO2 Storage With Spatio-Temporal CNNs","type":"publication"},{"authors":[],"categories":null,"content":"","date":1633032022,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633032022,"objectID":"b7208e8a176d6e4d08b2df16a879c440","permalink":"https://ZhangXit.github.io/talk/data-driven-seismic-inversion-learning-to-solve-inverse-problems-via-physics-informed-networks/","publishdate":"2023-10-10T15:18:22-04:00","relpermalink":"/talk/data-driven-seismic-inversion-learning-to-solve-inverse-problems-via-physics-informed-networks/","section":"event","summary":"Center for Nonlinear Studies Seminar.","tags":[],"title":"Data-Driven Seismic Inversion: Learning to Solve Inverse Problems via Physics-Informed Networks","type":"event"},{"authors":[],"categories":null,"content":"","date":1608062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608062400,"objectID":"08487a8f7fc7ebd3463ecc84ebcd4bf7","permalink":"https://ZhangXit.github.io/talk/earthquake-source-characterization-with-graphical-deep-learning/","publishdate":"2023-10-10T15:35:40-04:00","relpermalink":"/talk/earthquake-source-characterization-with-graphical-deep-learning/","section":"event","summary":"AGU 2020 Fall Meeting Oral Presentation.","tags":[],"title":"Earthquake Source Characterization with Graphical Deep Learning","type":"event"},{"authors":["Xitong Zhang","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://ZhangXit.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Xitong Zhang","He Zhu","Jiayu Zhou"],"categories":["1"],"content":"","date":1585928597,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585928597,"objectID":"8c97d4a58c00f956a8c91b621847c0ad","permalink":"https://ZhangXit.github.io/publication/shoreline/","publishdate":"2020-04-03T11:43:17-04:00","relpermalink":"/publication/shoreline/","section":"publication","summary":"Trading security threshold estimation by dynamic graph embedding.","tags":[],"title":"Shoreline: Data-Driven Threshold Estimation of Online Reserves of Cryptocurrency Trading Platforms","type":"publication"},{"authors":[],"categories":null,"content":"","date":1581277545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581277545,"objectID":"f8ec70e6231ec6df05cd912b34248c26","permalink":"https://ZhangXit.github.io/talk/shoreline-data-driven-threshold-estimation-of-online-reserves-of-cryptocurrency-trading-platforms-authors/","publishdate":"2023-10-10T15:08:45-04:00","relpermalink":"/talk/shoreline-data-driven-threshold-estimation-of-online-reserves-of-cryptocurrency-trading-platforms-authors/","section":"event","summary":"AAAI Technical Track Oral Presentation (AAAI-2020).","tags":[],"title":"Shoreline: Data-Driven Threshold Estimation of Online Reserves of Cryptocurrency Trading Platforms Authors","type":"event"},{"authors":[],"categories":null,"content":" ","date":1573380000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573380000,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://ZhangXit.github.io/talk/boosted-trajectory-calibration-for-traffic-state-estimation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/boosted-trajectory-calibration-for-traffic-state-estimation/","section":"event","summary":"Long Paper Oral Presentation (ICDM 2019).","tags":[],"title":"Boosted Trajectory Calibration for Traffic State Estimation","type":"event"},{"authors":["Xitong Zhang","Liyang Xie","Zheng Wang","Jiayu Zhou"],"categories":[],"content":"","date":1573228462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573228462,"objectID":"55e67e9c413a5828b986df71ab5549b8","permalink":"https://ZhangXit.github.io/publication/btrac/","publishdate":"2019-11-08T11:54:22-04:00","relpermalink":"/publication/btrac/","section":"publication","summary":"Traffic state estimation is among the most critical issues in intelligent transport systems, and it has been widely explored for years considering its significance for diverse industrial applications such as vehicle dispatch and route planning. The availability of vehicle trajectories has provided a promising data source for traffic estimation, as it shed light on traffic status and characterizes human transition activates that are otherwise extremely hard to track. However, there are major challenges in incorporating trajectories in traffic estimation largely because of its inherent uncertainty from data sparsity and semantic ambiguity. Entangled in complicated road topology and spatiotemporal traffic dynamics, properly embedding trajectories in modeling is especially difficult. To address the aforementioned challenges, in this paper, we propose a Boosted Trajectory Calibration (BTRAC) framework to model traffic states of complex road networks that effectively integrates trajectory information. In the framework, we first use a deep neural network to predict the traffic states of each road individually from historical traffic information, along with the prediction uncertainty. Then we refine the predictions by an iterative boosting calibration procedure with embedded trajectories. We conduct extensive large-scale empirical studies on two cities and demonstrate the effectiveness of the proposed approach.","tags":[],"title":"Boosted trajectory calibration for traffic state estimation","type":"publication"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://ZhangXit.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Xitong Zhang"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://ZhangXit.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ZhangXit.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1481328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481328000,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://ZhangXit.github.io/project/external-project/","publishdate":"2016-12-10T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"The system is designed to visualize the US historical feed grain data and gain insights into the supply and the demand of feed grains in domestic market and international trade of the US with import and export partners.","tags":["Demo"],"title":"FeedGrain Visualization","type":"project"}]